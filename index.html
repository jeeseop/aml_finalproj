<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Jeeseop Kim </title>
</head>
<body style="width: 70%">
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">menu</div>
<div class="menu-item"><a href="index.html" class="current">Overview</a></div>
<div class="menu-item"><a href="procedure.html">Procedure&Codes</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Advanced Machine Learning Final Project </h1>
</div>
<h2>Paper Introduction & Summarization</h2>
<p>Nowadays lots of neural networks, including deep neural network, have become increasingly effective at many difficult machine-learning tasks. One of the major parts of this application is natural language processing. In this field, every natural language source becomes the dataset that could be learned. In one research, Harry Potter book became the dataset of the natural language processing, and they proposed the convolutional neural network for predicting the magic spell which can happen right after the behavior description in the book [3]. However, there are also lots of researchs which shows the vulnerability of the neural network from attack using advarsarial example [4]. The development of the algorithm that finds the advarsarial examples  has inspired research on how to harden neural networks against these kinds of attacks. Defense algorithm also has been proposed and evaluated in lots of researches. Obfuscated Gradients [2] is one of the characteristic which could be in defense algorithm which can gives the false sense of security. In this paper [2], both defense causing obfuscated gradients and advarsarial example that can circumvent the obfuscated gradient in the defense are examined and developed respectively. Defensive distillation [5] is one recent defense algorithm proposed for hardening neural networks against adversarial examples. The proposed defensive distillation can be applied easily to any feed-forward neural network, and defeats existing attack algorithms and reduces their success probability from 95% to 0.5% with ease. Based on this defense algorithm, this paper [1] proposed 3 different new attack algorithm using L_0, L_2, L_inf distances metrics. This paper first shows the excellence of the proposed attack comparing with the previous approaches. Jacobian based Saliency Map Attack (JSMA) [6] is used as a comparing set using L_0 distance as a cost function of optimizer. Deepfool [7] is also introduced as a comparing set of attack technique optimized for the L_2 distance metric. Fast graident sign method [8] is used as a comparing set of optimizer using L_inf as a distance metric. </p>

<table class="imgtable"><tr><td>
<img src="photo/comparison.png" alt="alt text" width="750px" height="200px" /></td>
</tr></table>

<p> This paper also applied these attacks to defensive distillation and discover that distillation provides little security benefit over un-distilled networks when applying the propsed attacks. In the original work, increasing the temperature was found to consistently reduce attack success rate [1]. On MNIST, this goes from a 91% success rate at T=1 to a 24% success rate for T=5 and finally 0.5% success at T=100. Interestingly, the proposed attack did not degrade even though the distillation temperature increases from t=1 to t=100. The success rate is described in the table below. </p>

<table class="imgtable"><tr><td>
<img src="photo/distil1.png" alt="alt text" width="750px" height="150px" /></td>
</tr></table>

<p> Furthermore, the mean advarsarial distance does not change even though the distillation temperature becomes higher. Figure below shows that proposed attacks works in a good mannor even though the distillation temperature becomes higher which effectively degrade the performance of the previous attack approaches.</p>

<table class="imgtable"><tr><td>
<img src="photo/distil2.png" alt="alt text" width="280px" height="250px" /></td>
</tr></table>

<p>In short, this paper proposed efficient attack algorithms which are using L_0, L_2, and L_inf distance metric as a cost function of the optimizer, and showed that proposed algorithms defeat defensive distillation. Also this paper mention that their attacks more generally can be used to evaluate the efficacy of potential defenses. </p>

<h2>Measurements and Analysis When Attempting the origin Result</h2>
<ul>
<li><p>First trained MNIST and used the proposed advarsarial example to check if it can really distort the image to make a target label.</p>
</li>
<li><p>Second, trained CIFAR and used the proposed advarsarial example to chekc if it can really distort the image to make a target label.</p>
</li>
<li><p>Third, trained MNIST using distillation defense algorithm to make the security higher, and applied advarsarial example to see the effectiveness of the proposed attack algorithm</p>
</li>
<li><p>Fourth, trained CIFAR using distillation defense algorithm to make the security higher, and applied advarsarial example to see the effectiveness of the proposed attack algorithm.</p>
</li>
</ul>


<div class="infoblock">
<div class="blockcontent">
<h2>Reference</h2>
<ul>
<li><p><b>[1] </b>Carlini, Nicholas, and David Wagner. "Towards evaluating the robustness of neural networks." 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.</p>
</li>
<li><p><b>[2] </b>Athalye, Anish, Nicholas Carlini, and David Wagner. "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples." arXiv preprint arXiv:1802.00420 (2018).</p>
</li>
<li><p><b>[3] </b>Vilares, David, and Carlos Gómez-Rodríguez. "Harry Potter and the Action Prediction Challenge from Natural Language." arXiv preprint arXiv:1905.11037 (2019).</p>
</li>
<li><p><b>[4] </b>Szegedy, Christian, et al. "Intriguing properties of neural networks." arXiv preprint arXiv:1312.6199 (2013).</p>
</li>
<li><p><b>[5] </b>Papernot, Nicolas, et al. "Distillation as a defense to adversarial perturbations against deep neural networks." 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 2016.</p>
</li>
<li><p><b>[6] </b>Papernot, Nicolas, et al. "The limitations of deep learning in adversarial settings." 2016 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2016.</p>
</li>
<li><p><b>[7] </b>Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. "Deepfool: a simple and accurate method to fool deep neural networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.</p>
</li>
<li><p><b>[8] </b>Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint arXiv:1412.6572 (2014).</p>
</li>
</ul>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2019-12-05</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
